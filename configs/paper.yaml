config:
  seed: 10
  device: cuda
  root_dir: ./ # project root dir
  dataset_dir: ./dataset/rans_naca0012/incompressible/dataset # dataset dir
  split_dir: ./dataset/rans_naca0012/incompressible/dataset/h5_files # split dir
  output_dir: ./output/ # output dir
  # mesh_file: D:\TA\TA2\GAE-ROM\dataset\cylinder_vonkarman\Re200_cut\cylinder_D0.4.su2
  mesh_file: D:\TA\TA2\GAE-ROM\dataset\rans_naca0012\incompressible\unsteady_naca0012_mesh.su2
  data_sample: 1000 # number of data samples to use for training
  variable_scaling: true
  scaler_name: standard
  scaling_type: 4
  inverse_scaling: true
  dim_pde: 1
  variable: X

preprocessing:
  normalization_method: zscore # ['zscore', 'magnitude', robust]
  with_edge_features: true

conv_base: &conv_base
  type: Cheb
  act: ELU # ReLU, Tanh, Sigmoid
  head: 4
  dropout: 0
  kernel_size: 2
  K: 2
  dim: 2
  num_layers: 5
  is_batch_norm: false
  is_skip_connection: true

autoencoder: &autoencoder_base
  is_autoencoder: true
  act: ELU # ReLU, Tanh, Sigmoid
  dropout: 0
  latent_dim: 25

model:
  encoder:
    convolution_layers:
      <<: *conv_base
      hidden_channels: [1, 1, 1]
    autoencoder_encoder:
      <<: *autoencoder_base
      encoder_layers: [400, 200, 100]

  decoder:
    convolution_layers:
      <<: *conv_base
      hidden_channels: [1, 1, 1]
    autoencoder_decoder:
      <<: *autoencoder_base
      decoder_layers: [100, 200, 400]


  maptovec:
    is_maptovec: true
    layer_vec: [2, 100, 100, 100, 100, 100, 25]
    act: Tanh # ReLU, Tanh, Sigmoid

training:
  model_name: NACA0012_GMMConv_AE
  num_workers: 0
  epochs: 25
  batch_size: 1
  print_train: 1
  amp: false
  single_batch_run: false
  save_best_model: true
  lambda_map: 1
  lambda_surface: 0
  optimizer:
    type: AdamW
    learning_rate: 0.001
    weight_decay: 0.0001

  scheduler:
    type: MultiStepLR
    milestones: []
    gamma: 0.0001
    T_max: 100
    eta_min: 0.00001

  early_stopping:
    early_stopping: 10
    patience: 10

  loss:
    type: mse # [mse, rmse]

  metric:
    type: mse # [mse, rmse]


