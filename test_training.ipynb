{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (encoder_layer_0): Linear(in_features=493536, out_features=2048, bias=True)\n",
      "  (encoder_layer_0_relu): ReLU()\n",
      "  (encoder_layer_1): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (encoder_layer_1_relu): ReLU()\n",
      "  (encoder_layer_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  (encoder_layer_2_relu): ReLU()\n",
      "  (encoder_layer_3): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (encoder_layer_3_relu): ReLU()\n",
      "  (encoder_layer_4): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (encoder_layer_4_relu): ReLU()\n",
      "  (encoder_layer_latent_dim): Linear(in_features=16, out_features=2, bias=True)\n",
      "  (encoder_layer_latent_dim_relu): ReLU()\n",
      ")\n",
      "Sequential(\n",
      "  (decoder_layer_latent_dim): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (decoder_layer_latent_dim_relu): ReLU()\n",
      "  (decoder_layer_0): Linear(in_features=16, out_features=64, bias=True)\n",
      "  (decoder_layer_0_relu): ReLU()\n",
      "  (decoder_layer_1): Linear(in_features=64, out_features=256, bias=True)\n",
      "  (decoder_layer_1_relu): ReLU()\n",
      "  (decoder_layer_2): Linear(in_features=256, out_features=1024, bias=True)\n",
      "  (decoder_layer_2_relu): ReLU()\n",
      "  (decoder_layer_3): Linear(in_features=1024, out_features=2048, bias=True)\n",
      "  (decoder_layer_3_relu): ReLU()\n",
      "  (decoder_layer_output): Linear(in_features=2048, out_features=493536, bias=True)\n",
      "  (decoder_layer_output_relu): ReLU()\n",
      ")\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m GraphDataset()\n\u001b[0;32m     14\u001b[0m config \u001b[38;5;241m=\u001b[39m commons\u001b[38;5;241m.\u001b[39mget_config(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfigs/reduced.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# optimizer\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\'gca_rom'\\lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\'gca_rom'\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\'gca_rom'\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 780 (3 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\'gca_rom'\\lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\'gca_rom'\\lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\'gca_rom'\\lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1155\u001b[0m             device,\n\u001b[0;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m             non_blocking,\n\u001b[0;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[1;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Yuno\\anaconda3\\envs\\'gca_rom'\\lib\\site-packages\\torch\\cuda\\__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from src.training.train import train\n",
    "from src.model.gae import GAE\n",
    "from src.data.loader import GraphDataset\n",
    "from src.utils import commons\n",
    "\n",
    "device = commons.get_device()\n",
    "# device = 'cuda'\n",
    "torch.cuda.empty_cache()\n",
    "dataset = GraphDataset()\n",
    "config = commons.get_config('configs/reduced.yaml')\n",
    "model = GAE(config=config['model'], num_nodes = dataset.num_nodes).to(device)\n",
    "\n",
    "# optimizer\n",
    "if config['training']['optimizer']['type'] == 'Adam':\n",
    "    from torch.optim.adam import Adam\n",
    "    optimizer = Adam(model.parameters(), lr=config['training']['optimizer']['learning_rate'])\n",
    "elif config['training']['optimizer']['type'] == 'AdamW':\n",
    "    from torch.optim.adamw import AdamW\n",
    "    optimizer = AdamW(model.parameters(), lr=config['training']['optimizer']['learning_rate'])\n",
    "else:\n",
    "    raise ValueError(f\"Invalid optimizer: {config['training']['optimizer']['type']}\")\n",
    "\n",
    "# scheduler \n",
    "if config['training']['scheduler']['type'] == 'StepLR':\n",
    "    from torch.optim.lr_scheduler import StepLR\n",
    "    scheduler = StepLR(optimizer, step_size=config['training']['scheduler']['step_size'], gamma=config['training']['scheduler']['gamma'])\n",
    "elif config['training']['scheduler']['type'] == 'CosineAnnealingLR':\n",
    "    from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=config['training']['epochs'])\n",
    "else:\n",
    "    raise ValueError(f\"Invalid scheduler: {config['training']['scheduler']['type']}\")\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, \n",
    "                          batch_size=config['training']['batch_size'], \n",
    "                          shuffle=False,\n",
    "                          num_workers=config['config']['num_workers'])\n",
    "\n",
    "train_history = train(model=model, \n",
    "                      optimizer=optimizer,\n",
    "                      device=device,\n",
    "                      scheduler=scheduler, \n",
    "                      train_loader=train_loader, \n",
    "                      config=config['training'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [1,2,3,4,5]\n",
    "list[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TA\\TA2\\GAE-ROM\\src\\data\\loader.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(self.edge_list, dtype=torch.long),\n",
      "d:\\TA\\TA2\\GAE-ROM\\src\\data\\loader.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_attr = torch.tensor(self.edge_features, dtype=torch.float32),\n",
      "d:\\TA\\TA2\\GAE-ROM\\src\\data\\loader.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_weight = torch.tensor(self.edge_weights, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch_geometric.nn import GCNConv, ChebConv, GATConv, GMMConv\n",
    "from src.data.loader import GraphDataset\n",
    "import torch.nn as nn\n",
    "# Create dataset and get a batch\n",
    "dataset = GraphDataset()\n",
    "batch = dataset[0]\n",
    "\n",
    "# Try GCN with edge weights\n",
    "# test_model = GCNConv(2, 32)\n",
    "# batch.x = test_model(batch.x, batch.edge_index, batch.edge_weight)\n",
    "\n",
    "encoder = nn.ModuleList()\n",
    "decoder = nn.ModuleList()\n",
    "\n",
    "encoder.append(GCNConv(2, 32))\n",
    "decoder.append(GCNConv(32, 2))\n",
    "\n",
    "for i in range(3):\n",
    "    encoder.append(GCNConv(32 - 2*i, 32 - 2 * (i+1)))\n",
    "    decoder.append(GCNConv(32 - 2 * (i+1), 32 - 2*i))\n",
    "\n",
    "decoder = decoder[-1::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.3373, -4.2184],\n",
       "        [-2.5727, -3.5770],\n",
       "        [-2.7493, -3.7444],\n",
       "        ...,\n",
       "        [-3.0141, -5.2052],\n",
       "        [-4.0269, -5.0360],\n",
       "        [-4.8975, -5.9311]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for model in encoder:\n",
    "    batch.x = model(batch.x, batch.edge_index, batch.edge_weight)\n",
    "\n",
    "for model in decoder:\n",
    "    batch.x = model(batch.x, batch.edge_index, batch.edge_weight)\n",
    "# batch.x = test_module(batch.x, batch.edge_index, batch.edge_weight)\n",
    "batch.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn.pool as pooling\n",
    "\n",
    "topk = pooling.TopKPooling(in_channels=2, ratio=0.5, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TopKPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30846, 2])\n",
      "torch.Size([2, 46015])\n",
      "torch.Size([46015])\n",
      "torch.Size([3085, 2])\n",
      "torch.Size([2, 4424])\n",
      "torch.Size([4424])\n",
      "torch.Size([3085])\n",
      "torch.Size([3085])\n",
      "torch.Size([3085])\n"
     ]
    }
   ],
   "source": [
    "print(batch.x.shape)\n",
    "print(batch.edge_index.shape)\n",
    "print(batch.edge_attr.shape)\n",
    "pooled_x, pooled_edge_index, pooled_edge_attr, batch_index, perm, score = topk(x = batch.x, edge_index = batch.edge_index, edge_attr = batch.edge_attr)\n",
    "\n",
    "print(pooled_x.shape)\n",
    "print(pooled_edge_index.shape)\n",
    "print(pooled_edge_attr.shape)\n",
    "print(batch_index.shape)\n",
    "print(perm.shape)\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pooled_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpooled_x\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pooled_x' is not defined"
     ]
    }
   ],
   "source": [
    "pooled_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EdgePooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TA\\TA2\\GAE-ROM\\src\\data\\loader.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(self.edge_list, dtype=torch.long),\n",
      "d:\\TA\\TA2\\GAE-ROM\\src\\data\\loader.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_attr = torch.tensor(self.edge_features, dtype=torch.float32),\n",
      "d:\\TA\\TA2\\GAE-ROM\\src\\data\\loader.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_weight = torch.tensor(self.edge_weights, dtype=torch.float32))\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import EdgePooling\n",
    "from torch_geometric.nn import GCNConv, ChebConv, GATConv, GMMConv\n",
    "from src.data.loader import GraphDataset\n",
    "import torch.nn as nn\n",
    "# Create dataset and get a batch\n",
    "dataset = GraphDataset()\n",
    "batch = dataset[0]\n",
    "edgepool = EdgePooling(in_channels=2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.5045, 2.8414],\n",
       "        [4.1457, 2.4405],\n",
       "        [2.9354, 1.7456],\n",
       "        ...,\n",
       "        [2.3839, 1.3395],\n",
       "        [1.0981, 1.1352],\n",
       "        [1.5320, 1.6905]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.data.data.Data'>\n",
      "<class 'NoneType'>\n",
      "30846\n"
     ]
    }
   ],
   "source": [
    "print(type(batch))           # should be <class 'torch_geometric.data.data.Data'>\n",
    "print(type(batch.batch))     # should be <class 'torch.Tensor'>\n",
    "print(batch.x.size(0))     # e.g., [num_nodes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30846, 2])\n",
      "torch.Size([2, 46015])\n",
      "tensor(30845)\n",
      "torch.Size([46015])\n",
      "torch.Size([17306, 2])\n",
      "torch.Size([2, 46014])\n",
      "tensor(17305)\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(batch.x.shape)\n",
    "print(batch.edge_index.shape)\n",
    "print(max(batch.edge_index[1]))\n",
    "print(batch.edge_attr.shape)\n",
    "import torch\n",
    "\n",
    "# If your graph has N nodes, all belong to the same graph, so assign zeros:\n",
    "batch_vector = torch.zeros(batch.x.size(0), dtype=torch.long)\n",
    "\n",
    "pooled_x, pooled_edge_index, pooled_batch, unpool_info = edgepool(batch =  batch_vector, x = batch.x, edge_index = batch.edge_index)\n",
    "# pooled_result = edgepool(batch = batch_vector, x = batch.x, edge_index = batch.edge_index)\n",
    "\n",
    "print(pooled_x.shape)\n",
    "print(pooled_edge_index.shape)\n",
    "print(max(pooled_edge_index[1]))\n",
    "print(pooled_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[46.3758, -0.8608],\n",
       "        [38.7250, -1.5845],\n",
       "        [40.6678, -1.4719],\n",
       "        ...,\n",
       "        [28.8896, -2.3286],\n",
       "        [34.1928, -0.3318],\n",
       "        [34.1816, -0.2373]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0998e+02,  8.1582e-01],\n",
       "        [ 1.3889e+02, -2.8769e+00],\n",
       "        [ 1.1589e+02, -4.7028e+00],\n",
       "        ...,\n",
       "        [ 3.5362e+01, -6.6867e-02],\n",
       "        [ 3.5490e+01,  7.5209e-01],\n",
       "        [ 3.5759e+01,  1.4351e+00]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.plot import Plot\n",
    "\n",
    "plot = Plot(data_dir = \"data\", save_dir = \"output\")\n",
    "plot.plot_tensor(tensor = pooled_x[:,0], data_dim = (1, 2), title = \"Pooled X\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'gca_rom'",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
